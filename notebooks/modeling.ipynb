{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir(Path().resolve().parent)\n",
    "\n",
    "from src.data_processing.loader import SAMDataset, create_dataloader\n",
    "from src.modeling.trainer import SAMTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SAMDataset(\n",
    "    image_dir=\"data/train_images\",\n",
    "    mask_dir=\"data/train_labels\",\n",
    "    spacing_metadata_dir=\"data/metadata/spacing_mm.txt\",\n",
    "    processor=\"facebook/sam-vit-base\",\n",
    ")\n",
    "\n",
    "print(f\"Number of records: {len(train_dataset)}\")\n",
    "print(f\"Example of one record:\")\n",
    "for k, v in train_dataset[0].items():\n",
    "    print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 6543\n",
      "Example of one record:\n",
      "pixel_values: torch.Size([3, 1024, 1024])\n",
      "original_sizes: torch.Size([2])\n",
      "reshaped_input_sizes: torch.Size([2])\n",
      "input_boxes: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "test_dataset = SAMDataset(\n",
    "    image_dir=\"data/test_images\",\n",
    "    bbox_file_dir=\"data/metadata/test1_bbox.txt\",\n",
    "    spacing_metadata_dir=\"data/metadata/spacing_mm.txt\",\n",
    "    processor=\"facebook/sam-vit-base\",\n",
    ")\n",
    "\n",
    "print(f\"Number of records: {len(test_dataset)}\")\n",
    "print(f\"Example of one record:\")\n",
    "for k, v in test_dataset[0].items():\n",
    "    print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(\n",
    "    train_dataset,\n",
    "    batch_size=36,\n",
    "    train_ratio=0.8,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of one batch:\n",
      "pixel_values: torch.Size([36, 3, 1024, 1024])\n",
      "original_sizes: torch.Size([36, 2])\n",
      "reshaped_input_sizes: torch.Size([36, 2])\n",
      "input_boxes: torch.Size([36, 1, 4])\n",
      "ground_truth_mask: torch.Size([36, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "print(f\"Example of one batch:\")\n",
    "for k, v in batch.items():\n",
    "    print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SamModel(\n",
      "  (shared_image_embedding): SamPositionalEmbedding()\n",
      "  (vision_encoder): SamVisionEncoder(\n",
      "    (patch_embed): SamPatchEmbeddings(\n",
      "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x SamVisionLayer(\n",
      "        (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): SamVisionAttention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): SamMLPBlock(\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (neck): SamVisionNeck(\n",
      "      (conv1): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (layer_norm1): SamLayerNorm()\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (layer_norm2): SamLayerNorm()\n",
      "    )\n",
      "  )\n",
      "  (prompt_encoder): SamPromptEncoder(\n",
      "    (shared_embedding): SamPositionalEmbedding()\n",
      "    (mask_embed): SamMaskEmbedding(\n",
      "      (activation): GELUActivation()\n",
      "      (conv1): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (conv2): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (conv3): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (layer_norm1): SamLayerNorm()\n",
      "      (layer_norm2): SamLayerNorm()\n",
      "    )\n",
      "    (no_mask_embed): Embedding(1, 256)\n",
      "    (point_embed): ModuleList(\n",
      "      (0-3): 4 x Embedding(1, 256)\n",
      "    )\n",
      "    (not_a_point_embed): Embedding(1, 256)\n",
      "  )\n",
      "  (mask_decoder): SamMaskDecoder(\n",
      "    (iou_token): Embedding(1, 256)\n",
      "    (mask_tokens): Embedding(4, 256)\n",
      "    (transformer): SamTwoWayTransformer(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x SamTwoWayAttentionBlock(\n",
      "          (self_attn): SamAttention(\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          (cross_attn_token_to_image): SamAttention(\n",
      "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): SamMLPBlock(\n",
      "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (layer_norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          (layer_norm4): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          (cross_attn_image_to_token): SamAttention(\n",
      "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_attn_token_to_image): SamAttention(\n",
      "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "      )\n",
      "      (layer_norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (upscale_conv1): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (upscale_conv2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (upscale_layer_norm): SamLayerNorm()\n",
      "    (activation): GELU(approximate='none')\n",
      "    (output_hypernetworks_mlps): ModuleList(\n",
      "      (0-3): 4 x SamFeedForward(\n",
      "        (activation): ReLU()\n",
      "        (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (proj_out): Linear(in_features=256, out_features=32, bias=True)\n",
      "        (layers): ModuleList(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (iou_prediction_head): SamFeedForward(\n",
      "      (activation): ReLU()\n",
      "      (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (proj_out): Linear(in_features=256, out_features=4, bias=True)\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "trainer = SAMTrainer(\n",
    "    model_name=\"facebook/sam-vit-base\",  # Default model name\n",
    "    device=\"cpu\",  # Use GPU (cuda) if available\n",
    "    learning_rate=1e-5,  # Default learning rate\n",
    "    weight_decay=0,  # Default weight decay\n",
    ")\n",
    "print(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.k_fold_cross_validation(\n",
    "#     dataloader=train_dataloader,  # SAM DataLoader object \n",
    "#     k_folds=5,  # Default: 5 folds\n",
    "#     num_epochs=10,  # Default: 10 epochs per fold\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6543"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SamImageSegmentationOutput(iou_scores=tensor([[[0.9527, 0.9629, 0.9341]]]), pred_masks=tensor([[[[[-16.9070, -16.5040, -16.4959,  ..., -16.9824, -17.2585,\n",
       "            -17.1890],\n",
       "           [-16.8632, -16.9447, -16.6422,  ..., -17.2348, -17.5858,\n",
       "            -17.5795],\n",
       "           [-16.2116, -15.6282, -17.1129,  ..., -16.0656, -17.4824,\n",
       "            -17.0776],\n",
       "           ...,\n",
       "           [-17.4280, -17.9174, -17.2977,  ..., -17.8660, -18.1844,\n",
       "            -18.3659],\n",
       "           [-17.0276, -17.0331, -17.9977,  ..., -17.3907, -18.2660,\n",
       "            -17.6402],\n",
       "           [-17.2971, -17.0041, -17.6774,  ..., -17.4256, -18.0767,\n",
       "            -18.7077]],\n",
       "\n",
       "          [[-14.4527, -14.5858, -14.0422,  ..., -14.1928, -14.9696,\n",
       "            -14.9991],\n",
       "           [-14.3798, -15.0212, -14.0075,  ..., -14.1970, -15.1347,\n",
       "            -15.2938],\n",
       "           [-13.7320, -13.6956, -14.5075,  ..., -13.4944, -14.5137,\n",
       "            -14.5743],\n",
       "           ...,\n",
       "           [-15.2405, -16.3668, -14.9343,  ..., -15.1885, -15.7914,\n",
       "            -15.5148],\n",
       "           [-14.7671, -15.2788, -15.5205,  ..., -15.0325, -15.4080,\n",
       "            -15.3451],\n",
       "           [-14.8740, -15.3233, -15.0533,  ..., -14.7685, -15.3878,\n",
       "            -16.0194]],\n",
       "\n",
       "          [[-11.8529, -11.2792, -11.7851,  ..., -11.6872, -11.4731,\n",
       "            -11.1685],\n",
       "           [-11.8433, -11.8635, -11.8226,  ..., -11.9078, -11.5169,\n",
       "            -11.8368],\n",
       "           [-11.4560, -11.3418, -12.1070,  ..., -11.3303, -11.2355,\n",
       "            -10.9756],\n",
       "           ...,\n",
       "           [-12.2540, -12.7591, -12.0794,  ..., -13.3234, -12.2745,\n",
       "            -12.4817],\n",
       "           [-11.1522, -11.3335, -11.5420,  ..., -11.8118, -10.8333,\n",
       "            -11.1630],\n",
       "           [-12.0105, -12.2177, -12.1491,  ..., -12.7572, -12.3865,\n",
       "            -12.9528]]]]]), vision_hidden_states=None, vision_attentions=None, mask_decoder_attentions=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import SamModel, SamProcessor\n",
    "\n",
    "# # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "# model = trainer.model\n",
    "# processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n",
    "\n",
    "# record = test_dataset[0]\n",
    "# pixel_values = record[\"pixel_values\"].unsqueeze(0).to(device)\n",
    "# original_sizes = record[\"original_sizes\"].unsqueeze(0).to(device)\n",
    "# reshaped_input_sizes = record[\"reshaped_input_sizes\"].unsqueeze(0).to(device)\n",
    "# input_boxes = record[\"input_boxes\"].unsqueeze(0).to(device)\n",
    "\n",
    "# inputs = {\n",
    "#     \"pixel_values\": pixel_values,\n",
    "#     \"input_boxes\": input_boxes,\n",
    "# }\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 256, 256])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputs.pred_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # apply sigmoid\n",
    "# medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "# # convert soft mask to hard mask\n",
    "# medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
    "# medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
